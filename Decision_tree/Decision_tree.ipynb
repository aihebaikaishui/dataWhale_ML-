{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 任务3 - 决策树算法梳理】时长：3天（2019-05-18-晚10点）\n",
    "## 信息论基础（熵 联合熵 条件熵 信息增益 基尼不纯度） 2.决策树的不同分类算法（ID3算法、C4.5、CART分类树）的原理及应用场景\n",
    "## 回归树原理\n",
    "## 决策树防止过拟合手段\n",
    "## 模型评估\n",
    "## sklearn参数详解，Python绘制决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Solutions** 决策树\n",
    "\n",
    "[TOC]\n",
    "\n",
    "## 前言\n",
    "\n",
    "### 章节目录\n",
    "\n",
    "1. [模型与策略]决策树**模型**与**学习**\n",
    "   1. 决策树模型\n",
    "      1. 决策树与if-then规则\n",
    "      1. 决策树与条件概率分布\n",
    "   1. 决策树学习(三个步骤:特征选择，决策树生成，决策树修剪)\n",
    "1. ~~算法~~\n",
    "   1. [算法]特征选择\n",
    "      1. 特征选择问题(下面两个介绍了常用的准则，另外还有基尼系数在CART算法部分讲解)\n",
    "         1. 信息增益\n",
    "         1. 信息增益比\n",
    "   1. [算法]决策树的生成\n",
    "      1. ID3算法\n",
    "      1. C4.5的生成算法\n",
    "   1. [算法]决策树的剪枝\n",
    "   1. [算法]CART算法\n",
    "      1. CART生成\n",
    "      1. CART剪枝\n",
    "\n",
    "### 导读\n",
    "\n",
    "- 决策树是一种基本的分类与回归方法. 在书中CART算法之前的章节说的都是分类树，ID3和C4.5都只能处理分类问题，从CART(Classification and Regression Tree)开始有回归树，统称为决策树\n",
    "- 以上在章节目录部分添加了一部分标记，把这个章节按照模型，策略与算法进行划分，进一步重新整理了结构，希望可以帮助理清章节内容之间的关系\n",
    "- 在[CH12](../CH12/README.md)中有提到，决策树学习的损失函数是**对数似然损失**，关于决策树的剪枝，最重要的在于这个损失函数的理解。\n",
    "- 这个章节的主题是决策树，内容内涵和外延都很广，这个章节推荐阅读图灵社区的一个访谈[^1]，了解一下李航老师的故事，也可以对本章的最后三个参考文献[^2][^3][^4]有进一步的了解.\n",
    "- 引文中关于CART的介绍，是一本368页的书，2017年10月有了[Kindle版本](https://www.amazon.com/Classification-Regression-Trees-Leo-Breiman-ebook/dp/B076M7QKC6)，书的共同作者Friedman JH也是另一本神书ESL[参考文献7]的共同作者。\n",
    "- CART虽然在本书中排在ID3和C4.5后面，但是发表的时间顺序为CART->ID3->C4.5，了解决策树历史可以参考Loh的报告[^5]\n",
    "- 熵, 基尼指数衡量的都是集合的不确定性, 应用在推荐的场景, 不确定性越大, 覆盖率可能就越大.\n",
    "- 书中有提到，`分类问题中， 决策树表示基于特征对实例进行分类的过程。它可以认为是if-then规则的集合， 也可以认为是定义在特征空间上与类空间的条件概率分布。`书中第一小节对这个问题做了解释。\n",
    "- **剪枝**是模型压缩领域中的经典技术。剪枝可以降低模型复杂度，防止过拟合，提升模型泛化性能。LeCun等在1990年提出OBD[^8]方法对神经网络进行剪枝。\n",
    "\n",
    "## 概念\n",
    "\n",
    "### 熵\n",
    "\n",
    "$$\n",
    "H(p)=H(X)=-\\sum_{i=1}^{n}p_i\\log p_i\n",
    "$$\n",
    "\n",
    "**熵只与$X$的分布有关，与$X$取值无关**，这句注意理解\n",
    "\n",
    "定义$0\\log0=0$，熵是非负的。\n",
    "\n",
    "### 条件熵\n",
    "\n",
    "随机变量$(X,Y)$的联合概率分布为\n",
    "\n",
    "$P(X=x_i,Y=y_j)=p_{ij}, i=1,2,\\dots ,n;j=1,2,\\dots ,m$\n",
    "\n",
    "条件熵$H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性\n",
    "$$\n",
    "H(Y|X)=\\sum_{i=1}^np_iH(Y|X=x_i)\n",
    "$$\n",
    "其中$p_i=P(X=x_i),i=1,2,\\dots ,n$\n",
    "\n",
    "### 经验熵， 经验条件熵\n",
    "\n",
    "> 当熵和条件熵中的概率由数据估计(特别是极大似然估计)得到时，所对应的熵与条件熵分别称为经验熵和经验条件熵\n",
    "\n",
    "就是从已知的数据计算得到的结果。\n",
    "\n",
    "### 信息增益\n",
    "\n",
    "特征$A$对训练数据集$D$的信息增益$g(D|A)$，定义为集合$D$的经验熵$H(D)$与特征$A$给定的条件下$D$的经验条件熵$H(D|A)$之差\n",
    "$$\n",
    "g(D,A)=H(D)-H(D|A)\n",
    "$$\n",
    "\n",
    "熵与条件熵的差称为互信息.\n",
    "\n",
    "决策树中的信息增益等价于训练数据集中的类与特征的互信息。\n",
    "\n",
    "考虑ID这种特征， 本身是唯一的。按照ID做划分， 得到的经验条件熵为0, 会得到最大的信息增益。所以， 按照信息增益的准则来选择特征， 可能会倾向于取值比较多的特征。\n",
    "\n",
    "### 信息增益比\n",
    "\n",
    "$$\n",
    "g_R(D,A)=\\frac{g(D,A)}{H_A(D)}\\\\\n",
    "H_A(D)=-\\sum_{i=1}^n\\frac{D_i}{D}log_2\\frac{D_i}{D}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## 算法\n",
    "\n",
    "这部分内容，原始的[5.1数据](./Input/data_5-1.txt)中最后的标签也是是和否，表示树模型的时候，叶结点不是很明显，所以简单改了下[数据标签](./Input/mdata_5-1.txt)。对应同样的树结构，输出的结果如下\n",
    "\n",
    "```python\n",
    "# data_5-1.txt\n",
    "{'有自己的房子': {'否': {'有工作': {'否': {'否': None}, '是': {'是': None}}}, '是': {'是': None}}}\n",
    "# mdata_5-1.txt\n",
    "{'有自己的房子': {'否': {'有工作': {'否': {'拒绝': None}, '是': {'批准': None}}}, '是': {'批准': None}}}\n",
    "```\n",
    "\n",
    "### 先导\n",
    "\n",
    "#### 算法5.1 信息增益\n",
    "\n",
    "> 输入：训练数据集$D$和特征$A$\n",
    ">\n",
    "> 输出：特征$A$对训练数据集$D$的信息增益$g(D,A)$\n",
    ">\n",
    "> 1. 数据集$D$的经验熵$H(D)=-\\sum_{k=1}^K\\frac{|C_k|}{|D|}\\log_2\\frac{|C_k|}{|D|}$\n",
    "> 1.  特征$A$对数据集$D$的经验条件熵$H(D|A)=\\sum_{i=1}^n\\frac{|D_i|}{|D|}H(D_i)=-\\sum_{i=1}^n\\frac{|D_i|}{|D|}\\sum_{k=1}^K\\frac{|D_{ik}|}{|D_i|}\\log_2\\frac{|D_{ik}|}{|D_i|}$\n",
    "> 1. 信息增益$g(D,A)=H(D)-H(D|A)$\n",
    "\n",
    "\n",
    "\n",
    "#### 算法5.2 ID3算法\n",
    "\n",
    "> 输入：训练数据集$D$, 特征集$A$，阈值$\\epsilon$\n",
    "> 输出：决策树$T$\n",
    ">\n",
    "> 1. 如果$D$属于同一类$C_k$，$T$为单节点树，类$C_k$作为该节点的类标记，返回$T$\n",
    "> 1. 如果$A$是空集，置$T$为单节点树，实例数最多的类作为该节点类标记，返回$T$\n",
    "> 1. 计算$g$, 选择信息增益最大的特征$A_g$\n",
    "> 1. 如果$A_g$的信息增益小于$\\epsilon$，$T$为单节点树，$D$中实例数最大的类$C_k$作为类标记，返回$T$\n",
    "> 1. $A_g$划分若干非空子集$D_i$，\n",
    "> 1. $D_i$训练集，$A-A_g$为特征集，递归调用前面步骤，得到$T_i$，返回$T_i$\n",
    "\n",
    "\n",
    "\n",
    "#### 算法5.3 C4.5生成\n",
    "\n",
    "> 输入：训练数据集$D$, 特征集$A$，阈值$\\epsilon$\n",
    "> 输出：决策树$T$\n",
    ">\n",
    "> 1. 如果$D$属于同一类$C_k$，$T$为单节点树，类$C_k$作为该节点的类标记，返回$T$\n",
    "> 1. 如果$A$是空集, 置$T$为单节点树，实例数最多的作为该节点类标记，返回$T$\n",
    "> 1. 计算$g$, 选择**信息增益比**最大的特征$A_g$\n",
    "> 1. 如果$A_g$的**信息增益比**小于$\\epsilon$，$T$为单节点树，$D$中实例数最大的类$C_k$作为类标记，返回$T$\n",
    "> 1. $A_g$划分若干非空子集$D_i$，\n",
    "> 1. $D_i$训练集，$A-A_g$为特征集，递归调用前面步骤，得到$T_i$，返回$T_i$\n",
    "> ID3和C4.5在生成上，差异只在准则的差异。\n",
    "\n",
    "#### 算法5.4 树的剪枝\n",
    "\n",
    "决策树损失函数摘录如下：\n",
    "\n",
    "> 树$T$的叶结点个数为$|T|$，$t$是树$T$的叶结点，该结点有$N_t$个样本点，其中$k$类的样本点有$N_{tk}$个，$H_t(T)$为叶结点$t$上的经验熵， $\\alpha\\geqslant 0$为参数，决策树学习的损失函数可以定义为\n",
    "> $$\n",
    "> C_\\alpha(T)=\\sum_{i=1}^{|T|}N_tH_t(T)+\\alpha|T|\n",
    "> $$\n",
    "> 其中\n",
    "> $$\n",
    "> H_t(T)=-\\sum_k\\color{red}\\frac{N_{tk}}{N_t}\\color{black}\\log \\frac{N_{tk}}{N_t}\n",
    "> $$\n",
    ">\n",
    "> $$\n",
    "> C(T)=\\sum_{t=1}^{|T|}\\color{red}N_tH_t(T)\\color{black}=-\\sum_{t=1}^{|T|}\\sum_{k=1}^K\\color{red}N_{tk}\\color{black}\\log\\frac{N_{tk}}{N_t}\n",
    "> $$\n",
    ">\n",
    "> 这时有\n",
    "> $$\n",
    "> C_\\alpha(T)=C(T)+\\alpha|T|\n",
    "> $$\n",
    "> 其中$C(T)$表示模型对训练数据的误差，$|T|$表示模型复杂度，参数$\\alpha \\geqslant 0$控制两者之间的影响。\n",
    "\n",
    "上面这组公式中，注意红色部分，下面插入一个图\n",
    "\n",
    "![熵与概率的关系](assets/熵与概率的关系.png)\n",
    "\n",
    "这里面没有直接对$H_t(T)$求和，系数$N_t$使得$C(T)$和$|T|$的大小可比拟。这个地方再理解下。\n",
    "\n",
    "> 输入：生成算法生成的整个树$T$，参数$\\alpha$\n",
    ">\n",
    "> 输出：修剪后的子树$T_\\alpha$\n",
    ">\n",
    "> 1. 计算每个结点的经验熵\n",
    "> 1. 递归的从树的叶结点向上回缩\n",
    ">    假设一组叶结点回缩到其父结点之前与之后的整体树分别是$T_B$和$T_A$，其对应的损失函数分别是$C_\\alpha(T_A)$和$C_\\alpha(T_B)$，如果$C_\\alpha(T_A)\\leqslant C_\\alpha(T_B)$则进行剪枝，即将父结点变为新的叶结点\n",
    "> 1. 返回2，直至不能继续为止，得到损失函数最小的子树$T_\\alpha$\n",
    "\n",
    "```python\n",
    "{'有自己的房子': {'否': {'有工作': {'否': {'拒绝': None}, '是': {'批准': None}}}, '是': {'批准': None}}}\n",
    "```\n",
    "\n",
    "重新看一下这个建好的树，同样是字典的key，但是是有区别的。\n",
    "\n",
    "- 有自己的房子和有工作，是特征的索引\n",
    "\n",
    "- 是，否，是特征的取值\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "\tsubgraph 决策树\n",
    "\t\tA--是-->C[批准]\n",
    "\t\tA((有自己的房子))--否-->B((有工作))\n",
    "\t\tB--是-->D[批准]\n",
    "\t\tB--否-->E[拒绝]\n",
    "\tend\n",
    "\tsubgraph 子树\n",
    "\t\tB1((有工作))--是-->D1[批准]\n",
    "\t\tB1--否-->E1[拒绝]\n",
    "\tend\n",
    "\t\n",
    "```\n",
    "\n",
    "这个图里面，每一个划分都能计算经验熵\n",
    "\n",
    "```python\n",
    "ID\t年龄\t有工作\t有自己的房子\t信贷情况\t类别\n",
    "1\t青年\t否\t否\t一般\t拒绝\n",
    "2\t青年\t否\t否\t好\t拒绝\n",
    "15\t老年\t否\t否\t一般\t拒绝\n",
    "5\t青年\t否\t否\t一般\t拒绝\n",
    "6\t中年\t否\t否\t一般\t拒绝\n",
    "7\t中年\t否\t否\t好\t拒绝\n",
    "\t\t\t\t\t\n",
    "13\t老年\t是\t否\t好\t批准\n",
    "14\t老年\t是\t否\t非常好\t批准\n",
    "3\t青年\t是\t否\t好\t批准\n",
    "\t\t\t\t\t\n",
    "4\t青年\t是\t是\t一般\t批准\n",
    "8\t中年\t是\t是\t好\t批准\n",
    "9\t中年\t否\t是\t非常好\t批准\n",
    "10\t中年\t否\t是\t非常好\t批准\n",
    "11\t老年\t否\t是\t非常好\t批准\n",
    "12\t老年\t否\t是\t好\t批准\n",
    "\n",
    "```\n",
    "\n",
    "这里先不考虑书中提到的剪枝方案，从上面划分的过程，思考如何做前剪枝，显然可以通过控制最后的叶子节点的样本数量来控制，最后的样本数量越少，就越可能出现模型过分的去拟合训练样本中的数据。\n",
    "\n",
    "该章节代码中有这部分实现，在创建树的时候做预剪枝。\n",
    "\n",
    "```python\n",
    "def _min_samples_leaf_check(self,\n",
    "                            X):\n",
    "    items, cnts = np.unique(X, return_counts=True)\n",
    "    return np.min(cnts) < self.min_samples_leaf\n",
    "```\n",
    "\n",
    "剪枝，书中讲的算法是后剪枝，需要计算每个结点的经验熵，这个计算应该是在树构建的时候已经算过。\n",
    "\n",
    "这里面没有具体的实现例子，给出的参考文献是李航老师在CL上的文章，文章介绍的MDL是模型选择的一种具体框架，里面有介绍KL散度，这部分可以参考下。\n",
    "\n",
    "### CART\n",
    "\n",
    "关于CART的算法可以看下十大算法里面的第十章[^6]，一转眼就十大数据挖掘算法都发表十年了，这个本书第十章作者放在了ResearchGate上，链接见参考部分。\n",
    "\n",
    "#### 算法5.5 最小二乘回归树生成\n",
    "\n",
    "输入：训练数据集$D$\n",
    "\n",
    "输出：回归树$f(x)$\n",
    "\n",
    "步骤：\n",
    "\n",
    "1. 遍历变量$j$，对固定的切分变量$j$扫描切分点$s$，得到满足上面关系的$(j,s)$\n",
    "   $$\n",
    "   \\min\\limits_{j,s}\\left[\\min\\limits_{c_1}\\sum\\limits_{x_i\\in R_1(j,s)}(y_i-c_1)^2+\\min\\limits_{c_2}\\sum\\limits_{x_i\\in R_2(j,s)}(y_i-c_2)^2\\right]\n",
    "   $$\n",
    "\n",
    "1. 用选定的$(j,s)$, 划分区域并决定相应的输出值\n",
    "   $$\n",
    "   R_1(j,s)=\\{x|x^{(j)}\\leq s\\}, R_2(j,s)=\\{x|x^{(j)}> s\\} \\\\\n",
    "   \\hat{c}_m= \\frac{1}{N}\\sum\\limits_{x_i\\in R_m(j,s)} y_j, x\\in R_m, m=1,2\n",
    "   $$\n",
    "\n",
    "1. 对两个子区域调用(1)(2)步骤， 直至满足停止条件\n",
    "1. 将输入空间划分为$M$个区域$R_1, R_2,\\dots,R_M$，生成决策树：\n",
    "   $$\n",
    "   f(x)=\\sum_{m=1}^M\\hat{c}_mI(x\\in R_m)\n",
    "   $$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 算法5.6 CART分类树生成\n",
    "\n",
    "这个算法用到的策略是基尼系数，所以是分类树的生成算法。\n",
    "\n",
    ">  概率分布的基尼指数定义\n",
    "\n",
    "$$\n",
    "Gini(p) = \\sum_{k=1}^Kp_k(1-p_k)=1-\\sum_{k=1}^Kp_k^2\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "基尼系数是一个来源于经济学的指标. 范围(0, 1), 有很多中表示形式, 比如衡量收入分布的基尼系数.\n",
    "$$\n",
    "G=\\frac{1}{n-1}\\sum_{j=1}^n(2j-n-1)p(i_j)\n",
    "$$\n",
    "\n",
    "![](assets/Economics_Gini_coefficient2.svg)\n",
    "\n",
    "经济学基尼系数的解释[^7],基尼系数为$\\frac{A}{A+B}$\n",
    "\n",
    "#### 算法5.7 CART剪枝\n",
    "\n",
    "\n",
    "\n",
    "## 例子\n",
    "\n",
    "### 例5.1\n",
    "\n",
    "这个例子引出在特征选择的问题，后面跟着引出了熵，条件熵，信息增益与信息增益比的概念。这些是介绍决策树学习的基础。\n",
    "\n",
    "### 例5.2\n",
    "\n",
    "根据信息增益准则选择最优特征\n",
    "\n",
    "习题的5.1是让用信息增益比生成树，和这个基本一样，换一个准则就可以了。在单元测试里面实现了这部分代码。\n",
    "\n",
    "\n",
    "\n",
    "## 参考\n",
    "\n",
    "1. [^1]: [图灵社区李航访谈](http://www.ituring.com.cn/article/196610)\n",
    "\n",
    "1. [^2]: [山西健司]()\n",
    "\n",
    "1. [^3]: [决策列表, Text classification using ESC-based stochastic decision lists]()\n",
    "\n",
    "1. [^4]: [李航，安倍，CL文章，Generalizing case frames using a thesaurus and the MDL principle](http://www.aclweb.org/anthology/J98-2002)\n",
    "\n",
    "1. [^5]: [A Brief History of Classification and Regression Trees](http://washstat.org/presentations/20150604/loh_slides.pdf)\n",
    "\n",
    "1. [^6]: [The Top Ten Algorithms in Data Mining](https://www.researchgate.net/publication/265031802_Chapter_10_CART_Classification_and_Regression_Trees)\n",
    "\n",
    "1. [^7 ]: [Gini Coefficient](https://en.wikipedia.org/wiki/Gini_coefficient#/media/File:Economics_Gini_coefficient2.svg)\n",
    "\n",
    "1. [^8 ]: [Optimal brain damage](http://yann.lecun.com/exdb/publis/pdf/lecun-90b.pdf)\n",
    "\n",
    "**[⬆ top](#导读)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
